introduct ieeeparstart compress sens cs recent emerg promis approach acquir approxim spars signal import problem cs find sparsest solut follow determin linear system sampl matrix spars signal measur may contain nois equival written optim problem denot number nonzero entri vector nois level due nonsmooth nonconvex structur problem challeng find sparsest solut wide accept convex relax provid satisfactori approxim solut solut sampl matrix satisfi certain condit three differ version convex relax receiv lot attent basi pursuit denois bpdn regular least squar problem least absolut shrinkag select oper lasso model regular paramet tune paramet respect shown paramet chosen proper problem minim paper interest fast solut regular least squar model last year larg number algorithm develop problem list exemplari method refer review comprehens overview gradient type method e g gradient project spars reconstruct spars reconstruct via separ approxim spectral gradient project fix point iter continu strategi iter shrink threshold algorithm acceler extens extrem popular classic method e g homotopi method altern direct method multipli iter reweight least squar method also receiv reviv interest solv minim problem algorithm sublinear linear converg rate therefor immens interest develop newton type algorithm enjoy local superlinear converg rate invert matrix primal dual activ set pdas method also known semismooth newton method studi idea extend cs set solv problem theoret enjoy local superlinear converg howev newton type algorithm good initi guess import success applic pdas method unlik gradient base algorithm pdas method monoton decreas properti cost function therefor without good initi guess algorithm may converg meanwhil model regular paramet balanc sparsiti solut fidel measur proper choic play essenti role get satisfactori reconstruct articl propos simpl effici techniqu find good initi guess combin continu strategi primal dual activ set algorithm moreov equip proper stop rule regular paramet chosen automat without much ad work precis regular minim problem solv warm start predefin decreas sequenc e solut problem chosen initi guess problem solv pdas need newton step sinc provid good initi guess main contribut paper twofold first deriv local one step converg result problem improv well known local supperlin converg pdas import prove global converg primal dual activ set algorithm continu pdasc standard restrict isometri properti rip assumpt matrix nois free case hand measur involv nois adopt paramet select rule base either modifi discrep principl bayesian inform criterion one use rule select suitabl regular paramet solut continu process near ad effort rest paper organ follow section introduc mathemat background pdas algorithm continu techniqu discuss converg properti regular paramet select rule section sever numer exampl present illustr effici accuraci pdasc algorithm compar sever state art spars reconstruct algorithm technic proof appendic
