pdasc algorithm notat given vector denot denot transpos norm matrix respect matrix assum columnwis normal e notat refer column vector entri equal set size subvector submatrix whose entri column list denot set proper lower semicontinu convex function subdifferenti set valu map defin subdifferenti pointwis set valu sign function e classic fermat rule proper lower semicontinu convex function assert given proxim oper defin hold proxim oper given pointwis soft threshold oper motiv pdas algorithm character minim kkt system c f motiv pdas algorithm see also appendix short proof includ complet theorem minim exist kkt system hold convers satisfi minim let optim primal dual variabl clear follow henc one use inform primal dual variabl rather primal variabl alon determin nonzero compon call activ set motiv us defin activ inact set kkt system reformul first soft threshold oper deduc meanwhil proof theorem impli get e upon relabel equival written view relat rewritten henc activ set known optim solut follow direct motiv pdas algorithm suppos approxim similar defin activ inact set hope activ set inact set also good approxim respect repeat argument lead updat follow system clear involv matrix vector multipl thus comput effici well posed system depend solvabl turn depend properti submatrix compress sens problem activ set often small approxim also small like full column rank matrix discuss well posed subsect e summar pdas method algorithm algorithm h pdas input initi guess algorithm state comput state state state state state check stop rule either state output approxim endfor state complex analysi first consid number float point oper per iter clear take flop finish step pdas step form matrix explicit take flop cost form right hand side neglig sinc precomput retriev effici choleski factor cost flop back substitut need flop henc step take flop step two matrix vector product cost flop overal cost pdas per iter next issu number iter sinc pdas equival semi smooth newton method local superlinear converg guarante numer experi section also indic converg within iter good initi guess overal cost pdas also sought solut suffici spars e cost per pdas iter popular gradient base algorithm moreov even solut spars cost per pdas iter often appli choleski date precis downdat remov column cost flop updat append column flop cost choleski factor warm start differ small henc larg usual hold remark algorithm requir explicit form often signal spars compress certain basi sens matrix product random sampl matrix transform matrix e given implicit one avoid explicit express solv linear system step iter e g conjug gradient method cg involv matrix vector multipl often carri effici structur cg iter need due well conditioned system continu techniqu view equival pdas semismooth newton method good initi guess essenti success nonsmooth optim problem sever way global newton method includ squar smooth line search path follow model function detect due special structur cs problem adopt continu techniqu specif consid decreas sequenc paramet appli algorithm problem initi guess solut problem summar idea lead algorithm algorithm h pdasc input algorithm state set state find state algorithm check stop rule output approxim state endfor state converg analysi first consid converg algorithm local superlinear converg pdas obtain reformul semismooth newton framework problem cs set show stronger result local one step converg theorem let solut kkt system suppos set larg sens full column rank initi guess close enough generat algorithm ieeeproof see appendix b remark assumpt close relat sourc condit minim problem show global converg algorithm let true signal support activ set measur nois free e length activ set denot matrix satisfi restrict isometri properti rip order constant smallest constant hold assumpt satisfi rip order rip constant theorem let assumpt fulfil choic algorithm algorithm algorithm well defin suffici larg support ieeeproof see appendix c remark theorem consid nois free case noisi case nois level small algorithm still well defin equip suitabl stop rule ensur algorithm stop finit iter practic necessari larg assumpt slight differ constant use proof converg orthogon match pursuit algorithm omp select regular paramet discuss stop rule line algorithm choic regular paramet nois level known discrep principl wide appli choos suitabl regular paramet invers problem howev cs problem tend choos solut larg activ set see numer exampl section attribut fact regular model may lead bias solut precis suppos true activ set found thus primal dual variabl satisfi impli residu term may small henc discrep principl may satisfi meanwhil let oracl solut activ set hold henc better approxim true solut motiv us propos modifi discrep principl mdp stop rule select regular paramet specif let activ set algorithm algorithm stop nois level accord approxim solut given equat debia step see also similar debia postprocess one notic debia postprocess solut obtain may solut close solut minim problem debia postprocess done modifi discrep principl mdp satisfi nois level unknown choos stop criterion line algorithm size activ set e g choos proper regular paramet employ bayesian inform criterion bic data driven method wide use statist due model select consist bic choos solut subset repres degre freedom chosen due complex structur bic function nontrivi find minim whole posit real line instead practic way find minim finit candid set specifi next section numer test
